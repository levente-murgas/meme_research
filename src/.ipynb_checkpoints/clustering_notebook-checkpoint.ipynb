{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from cuml import HDBSCAN\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " import cudf\n",
    " print(cudf.Series([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labeled_embedding_output_dir = \"D:/embeddings\"\n",
    "unlabeled_embedding_output_dir = \"D:/embeddings\"\n",
    "\n",
    "labeled_phases_files = \"C:/Users/Murgi/Documents/GitHub/meme_research/src/pHash/labeled_phashes.txt\"\n",
    "unlabeled_phases_files = \"C:/Users/Murgi/Documents/GitHub/meme_research/src/pHash/unlabeled_phashes.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/Murgi/Documents/GitHub/meme_research/src/pHash/labeled_phashes.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 55\u001b[0m\n\u001b[1;32m     51\u001b[0m     unlabeled_embeddings, _, image_paths \u001b[38;5;241m=\u001b[39m load_embeddings(unlabeled_embedding_output_dir)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Load the phashes\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     labeled_embeddings, labeled_templates, _ \u001b[38;5;241m=\u001b[39m \u001b[43mload_phases\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabeled_phases_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     labeled_embeddings \u001b[38;5;241m=\u001b[39m phash_to_bin(labeled_embeddings)\n\u001b[1;32m     57\u001b[0m     unlabeled_embeddings, _, image_paths \u001b[38;5;241m=\u001b[39m load_phases(unlabeled_phases_files)\n",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m, in \u001b[0;36mload_phases\u001b[0;34m(phases_files)\u001b[0m\n\u001b[1;32m     23\u001b[0m class_names \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Open the file containing the phashes\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mphases_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Read the file into a list of lines\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# Split the line into the filename and phash\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.04/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Murgi/Documents/GitHub/meme_research/src/pHash/labeled_phashes.txt'"
     ]
    }
   ],
   "source": [
    "phash = True\n",
    "\n",
    "def load_embeddings(embedding_output_dir):\n",
    "    # Load all the embeddings into a single numpy array\n",
    "    files = glob.glob(embedding_output_dir + '/*.npy')\n",
    "    class_names = []\n",
    "    embeddings_list = []  # create a list to hold embeddings\n",
    "    for i, file in enumerate(files):\n",
    "        #Get filename\n",
    "        filename = file.split('\\\\')[-1]\n",
    "        class_name = filename.split('_')[1]\n",
    "        class_names.append(class_name)\n",
    "        embedding = np.load(file)\n",
    "        embeddings_list.append(embedding)\n",
    "    # Create a numpy array to hold the class names\n",
    "    class_names = np.array(class_names)\n",
    "    embeddings = np.concatenate(embeddings_list)\n",
    "    return embeddings, class_names\n",
    "\n",
    "def load_phases(phases_files):\n",
    "    phashes = []\n",
    "    imgage_paths = []\n",
    "    class_names = []\n",
    "    # Open the file containing the phashes\n",
    "    with open(phases_files, 'r') as f:\n",
    "        # Read the file into a list of lines\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        # Split the line into the filename and phash\n",
    "        path, phash = line.strip().split('\\t')\n",
    "        imgage_paths.append(path)\n",
    "        parts = os.path.normpath(path).split(os.sep)\n",
    "        if \"finetuning\" in parts:\n",
    "            #Get the dir of the image\n",
    "            class_name = path.split('\\\\')[-2]\n",
    "        else:\n",
    "            class_name = parts[-2]\n",
    "        # Get the class name from the path\n",
    "        class_names.append(class_name)\n",
    "        phashes.append(phash)\n",
    "    return np.array(phashes), np.array(class_names), np.array(imgage_paths)\n",
    "\n",
    "def phash_to_bin(phashes):\n",
    "    binary_repr = [bin(int(phash, 16))[2:].zfill(64) for phash in phashes]\n",
    "    return np.array([[int(bit) for bit in bin_str] for bin_str in binary_repr])\n",
    "\n",
    "if not phash:\n",
    "    # Load the embeddings\n",
    "    labeled_embeddings, labeled_templates, _ = load_embeddings(labeled_embedding_output_dir)\n",
    "    print(labeled_embeddings)\n",
    "    unlabeled_embeddings, _, image_paths = load_embeddings(unlabeled_embedding_output_dir)\n",
    "\n",
    "else:\n",
    "    # Load the phashes\n",
    "    labeled_embeddings, labeled_templates, _ = load_phases(labeled_phases_files)\n",
    "    labeled_embeddings = phash_to_bin(labeled_embeddings)\n",
    "    unlabeled_embeddings, _, image_paths = load_phases(unlabeled_phases_files)\n",
    "    unlabeled_embeddings = phash_to_bin(unlabeled_embeddings)\n",
    "\n",
    "print(\"finished loading\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Cluster the unlabeled data\n",
    "clusterer = HDBSCAN(min_cluster_size=2, metric='hamming')\n",
    "unlabeled_clusters = clusterer.fit_predict(unlabeled_embeddings)\n",
    "print(clusterer.labels_)\n",
    "\n",
    "# Get the unique cluster labels, excluding -1\n",
    "unique_clusters = np.unique(unlabeled_clusters[unlabeled_clusters != -1])\n",
    "\n",
    "# Initialize a list to hold the label for each cluster\n",
    "cluster_labels = [None] * len(np.unique(unique_clusters))\n",
    "\n",
    "# Initialize a list to hold the labels for all labeled memes\n",
    "assigned_labels = [None] * len(labeled_embeddings)\n",
    "\n",
    "# Initialize a list to hold the confidence for each cluster label\n",
    "confidence_scores = [None] * len(np.unique(unique_clusters))\n",
    "\n",
    "# Calculate the centroid for each cluster\n",
    "centroids = [np.mean(unlabeled_embeddings[unlabeled_clusters == cluster], axis=0) for cluster in unique_clusters]\n",
    "# Step 2: Assign labeled data to closest cluster\n",
    "for i, embedding in enumerate(labeled_embeddings):\n",
    "    distances = np.linalg.norm(centroids - embedding, axis=1)\n",
    "    assigned_labels[i] = np.argmin(distances)\n",
    "\n",
    "# Step 3: Determine cluster labels and confidence scores\n",
    "for i, cluster in enumerate(unique_clusters):\n",
    "    # Get the templates assigned to the cluster\n",
    "    assigned_templates = labeled_templates[np.array(assigned_labels) == cluster]\n",
    "    \n",
    "    # Count the occurrences of each template in the cluster\n",
    "    template_counts = Counter(assigned_templates)\n",
    "\n",
    "    # Assign the most common template as the label for the cluster\n",
    "    cluster_labels[i] = template_counts.most_common(1)[0][0]\n",
    "\n",
    "    # Calculate the confidence score for the label\n",
    "    confidence_scores[i] = template_counts.most_common(1)[0][1] / len(assigned_templates)\n",
    "\n",
    "# Step 4: Gather image paths for each cluster and noise\n",
    "cluster_image_paths = []\n",
    "for cluster in np.unique(unlabeled_clusters):\n",
    "    cluster_image_indices = np.where(unlabeled_clusters == cluster)[0]\n",
    "    cluster_image_paths.append(image_paths[cluster_image_indices].tolist())\n",
    "\n",
    "# Step 5: Save results to JSON file\n",
    "with open('cluster_results.json', 'w') as json_file:\n",
    "    cluster_data = []\n",
    "    for i, cluster in enumerate(np.unique(unlabeled_clusters)):\n",
    "        if cluster == -1:\n",
    "            cluster_dict = {\n",
    "                \"cluster_no\": str(cluster),\n",
    "                \"template_label\": \"noise\",\n",
    "                \"confidence_score\": \"NaN\",\n",
    "                \"images\": cluster_image_paths[i]\n",
    "            }\n",
    "        else:\n",
    "            cluster_dict = {\n",
    "                \"cluster_no\": str(cluster),\n",
    "                \"template_label\": cluster_labels[unique_clusters.tolist().index(cluster)],\n",
    "                \"confidence_score\": str(confidence_scores[unique_clusters.tolist().index(cluster)]),\n",
    "                \"images\": cluster_image_paths[i]\n",
    "            }\n",
    "        cluster_data.append(cluster_dict)\n",
    "    json.dump(cluster_data, json_file, indent=4)\n",
    "print(cluster_labels)\n",
    "print(confidence_scores)\n",
    "\n",
    "\n",
    "np.save('cluster_labels.npy', cluster_labels)\n",
    "np.save('confidence_scores.npy', confidence_scores)\n",
    "np.save('unlabeled_clusters.npy', unlabeled_clusters)\n",
    "np.save('assigned_labels.npy', assigned_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
