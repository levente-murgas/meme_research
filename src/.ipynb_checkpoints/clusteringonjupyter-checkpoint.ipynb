{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2194cc0-97e1-45ac-be86-26029187a333",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hdbscan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimg_to_vec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Img2Vec\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhdbscan\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mumap\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hdbscan'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from shutil import copyfile\n",
    "from utils import load_model\n",
    "from matplotlib import pyplot as plt\n",
    "from datasets import get_dataloaders\n",
    "# from img2vec_pytorch import Img2Vec\n",
    "from img_to_vec import Img2Vec\n",
    "from tqdm import tqdm\n",
    "from cuml.cluster import HDBSCAN\n",
    "from cuml.manifold import UMAP\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from numba import NumbaDeprecationWarning, NumbaPendingDeprecationWarning\n",
    "from sklearn.metrics import adjusted_rand_score, adjusted_mutual_info_score, fowlkes_mallows_score\n",
    "import plotly.express as px\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Measure time taken for the whole process\n",
    "    start_time = time.time()\n",
    "    warnings.simplefilter('ignore', category=NumbaDeprecationWarning)\n",
    "    warnings.simplefilter('ignore', category=NumbaPendingDeprecationWarning)\n",
    "    RETURN_VECTOR_LENGTH = {\n",
    "        'ResNet': 2048,\n",
    "        'AlexNet': 4096,\n",
    "        'VGG': 4096,\n",
    "        'DenseNet': 1024,\n",
    "        'EfficientNet': 1792\n",
    "    }\n",
    "\n",
    "    model_name = 'AlexNet'\n",
    "    embedding_output_dir = \"D:/embeddings\"\n",
    "\n",
    "    # Load the trained model.\n",
    "    model, input_size = load_model(model_name, feature_extract=True, on_all_data=True)\n",
    "\n",
    "    img2vec = Img2Vec(model=model,input_size=input_size, cuda=True)\n",
    "    vec_length = RETURN_VECTOR_LENGTH[model_name] \n",
    "\n",
    "    train_class_counts = pd.read_csv('C:/Users/Murgi/Documents/GitHub/meme_research/outputs/train_file_counts.csv')\n",
    "    train_class_counts = {row['class']: row['count'] for row in train_class_counts.to_dict(orient='records')}\n",
    "    num_classes = len(train_class_counts)\n",
    "\n",
    "    val_class_counts = pd.read_csv('C:/Users/Murgi/Documents/GitHub/meme_research/outputs/val_file_counts.csv')\n",
    "    val_class_counts = {row['class']: row['count'] for row in val_class_counts.to_dict(orient='records')}\n",
    "\n",
    "    train_val_class_counts_dict = {\n",
    "        'train': train_class_counts,\n",
    "        'val': val_class_counts\n",
    "    }\n",
    "\n",
    "    batch_size = 32\n",
    "    combined_dataloader = get_dataloaders(train_val_class_counts_dict=train_val_class_counts_dict,input_size=input_size, batch_size=batch_size, combined=True)\n",
    "\n",
    "    samples = len(combined_dataloader.dataset)\n",
    "\n",
    "    # Matrix to hold the image vectors\n",
    "    # vec_mat = np.zeros((samples, vec_length))\n",
    "\n",
    "    # Retrieve the last saved vector index and label\n",
    "    last_saved = '54432_3082_5.npy'\n",
    "    last_saved_index, last_saved_label, _ = last_saved.strip('.npy').split('_')\n",
    "    last_saved_index, last_saved_label = int(last_saved_index), int(last_saved_label)\n",
    "\n",
    "\n",
    "    print('Reading images...')\n",
    "    cnt = 0\n",
    "    resume = True\n",
    "    for inputs, labels in tqdm(combined_dataloader):\n",
    "        # Skip the images that have already been processed\n",
    "        if resume:\n",
    "            if cnt + len(inputs) <= last_saved_index:\n",
    "                cnt += len(inputs)\n",
    "                continue\n",
    "            else:\n",
    "                resume = False\n",
    "                inputs = inputs[last_saved_index - cnt:]\n",
    "                labels = labels[last_saved_index - cnt:]\n",
    "                cnt = last_saved_index\n",
    "        \n",
    "        # Get the image vectors\n",
    "        vecs = img2vec.get_vec(inputs, tensor=False)\n",
    "        # Save the vectors to disk as float16\n",
    "        for i, vec in enumerate(vecs):\n",
    "            np.save(os.path.join(embedding_output_dir, f'({cnt}_{labels[i]}_{i}.npy'), vec.astype(np.float16))\n",
    "        cnt += len(vecs)\n",
    "\n",
    "\n",
    "    filepaths = sorted(glob(os.path.join(embedding_output_dir, '*.npy')))\n",
    "    vecs = np.concatenate([np.load(fp) for fp in tqdm(filepaths)], axis=0)\n",
    "\n",
    "    print('Preprocessing...')\n",
    "    reducer = umap.UMAP(n_neighbors=40, min_dist=0.0, n_components=50, random_state=42)\n",
    "    reduced_feature_vectors = reducer.fit_transform(vecs)\n",
    "\n",
    "    print('Clustering...')\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=20)\n",
    "    cluster_labels = clusterer.fit_predict(reduced_feature_vectors)\n",
    "\n",
    "    # print('Cluster analysis...')\n",
    "    # print(\"Extrinsic Evaluation:\")\n",
    "    # ari = adjusted_rand_score(ground_truth, cluster_labels)\n",
    "    # print(\"Adjusted Rand Index:\", ari)\n",
    "    # ami = adjusted_mutual_info_score(ground_truth, cluster_labels)\n",
    "    # print(\"Adjusted Mutual Information:\", ami)\n",
    "    # fmi = fowlkes_mallows_score(ground_truth, cluster_labels)\n",
    "    # print(\"Fowlkes-Mallows Index:\", fmi)\n",
    "\n",
    "\n",
    "    print('Reducing dimensions for visualization...')\n",
    "    reducer = umap.UMAP(n_neighbors=40, min_dist=0.0, n_components=2, random_state=42)\n",
    "    embedding = reducer.fit_transform(reduced_feature_vectors)\n",
    "\n",
    "    print('Statistics:')\n",
    "    # Get the cluster labels from the fitted HDBSCAN object\n",
    "    cluster_labels = clusterer.labels_\n",
    "\n",
    "    # Calculate the number of clusters\n",
    "    num_clusters = len(np.unique(cluster_labels)) - 1  # Subtract 1 to account for the noise cluster (-1)\n",
    "\n",
    "    # Calculate the number of noise points\n",
    "    num_noise_points = np.sum(cluster_labels == -1)\n",
    "\n",
    "    # Calculate the noise percentage\n",
    "    noise_percentage = (num_noise_points / len(cluster_labels)) * 100\n",
    "\n",
    "    print(f\"Number of clusters formed: {num_clusters}\")\n",
    "    print(f\"Number of noise points: {num_noise_points}\")\n",
    "    print(f\"Noise percentage: {noise_percentage:.2f}%\")\n",
    "\n",
    "    # print('Plotting...')\n",
    "    # fig = px.scatter(\n",
    "    #     x=embedding[:, 0],\n",
    "    #     y=embedding[:, 1],\n",
    "    #     color=cluster_labels,\n",
    "    #     hover_name=image_filenames,\n",
    "    #     labels={'x': 'UMAP 1', 'y': 'UMAP 2', 'color': 'Cluster'},\n",
    "    #     title='UMAP projection of the dataset, colored by HDBSCAN clusters',\n",
    "    #     color_continuous_scale='Viridis',\n",
    "    #     width=800,\n",
    "    #     height=600,\n",
    "    # )\n",
    "\n",
    "    fig.show()\n",
    "    fig.write_html('C:/Users/Murgi/Documents/GitHub/meme_research/outputs/umap.html')\n",
    "    # Check time taken\n",
    "    print(f'Time taken: {time.time() - start_time:.2f} seconds')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
