{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from pybktree import BKTree\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the data exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found\n",
      "File found\n",
      "File found\n",
      "File found\n"
     ]
    }
   ],
   "source": [
    "labeled_embeddings_file = \"../outputs/cache/embeddings.pkl\"\n",
    "unlabeled_embeddings_file = \"../outputs/cache/reddit_embeddings_alexnet.pkl\"\n",
    "\n",
    "labeled_phases_files = \"./pHash/labeled_phashes.txt\"\n",
    "unlabeled_phases_files = \"./pHash/unlabeled_phashes.txt\"\n",
    "\n",
    "if os.path.exists(labeled_phases_files):\n",
    "    print(\"File found\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Labeled phash file not found\")\n",
    "\n",
    "if os.path.exists(unlabeled_phases_files):\n",
    "    print(\"File found\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Unlabeled phash file not found\")\n",
    "\n",
    "if os.path.exists(labeled_embeddings_file):\n",
    "    print(\"File found\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Labeled embeddings file not found\")\n",
    "\n",
    "if os.path.exists(unlabeled_embeddings_file):\n",
    "    print(\"File found\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"Unlabeled embeddings file not found\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "If phash is True, we load the phash data, otherwise we load the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading\n",
      "<class 'numpy.ndarray'>\n",
      "[[0 0 0 ... 1 0 1]\n",
      " [0 1 1 ... 1 0 1]\n",
      " [1 0 0 ... 1 1 0]\n",
      " ...\n",
      " [0 1 1 ... 0 1 0]\n",
      " [1 1 1 ... 0 1 0]\n",
      " [1 1 1 ... 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "phash = True\n",
    "\n",
    "def load_embeddings(embedding_file, unlabeled=False):\n",
    "    df = pd.read_pickle(embedding_file)\n",
    "    if unlabeled:\n",
    "        path = df['path'].to_numpy()\n",
    "        embeddings = np.vstack(df['embedding'].apply(np.array).to_numpy()).astype(np.float16)\n",
    "        return embeddings, None, path\n",
    "    else:\n",
    "        path = df['path'].to_numpy()\n",
    "        class_names = df['class_name'].to_numpy()\n",
    "        embeddings = np.vstack(df['embedding'].apply(np.array).to_numpy()).astype(np.float16)\n",
    "        return embeddings, class_names, path\n",
    "\n",
    "    \n",
    "def load_phases(phases_files):\n",
    "    phashes = []\n",
    "    imgage_paths = []\n",
    "    class_names = []\n",
    "    # Open the file containing the phashes\n",
    "    with open(phases_files, 'r') as f:\n",
    "        # Read the file into a list of lines\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        # Split the line into the filename and phash\n",
    "        path, phash = line.strip().split('\\t')\n",
    "        imgage_paths.append(path)\n",
    "        parts = os.path.normpath(path).split(os.sep)\n",
    "        if \"finetuning\" in parts:\n",
    "            #Get the dir of the image\n",
    "            class_name = path.split('\\\\')[-2]\n",
    "        else:\n",
    "            class_name = parts[-2]\n",
    "        # Get the class name from the path\n",
    "        class_names.append(class_name)\n",
    "        phashes.append(phash)\n",
    "    return np.array(phashes), np.array(class_names), np.array(imgage_paths)\n",
    "\n",
    "def phash_to_bin(phashes):\n",
    "    binary_repr = [bin(int(phash, 16))[2:].zfill(64) for phash in phashes]\n",
    "    return np.array([[int(bit) for bit in bin_str] for bin_str in binary_repr], dtype=np.uint8)\n",
    "\n",
    "if not phash:\n",
    "    # Load the embeddings\n",
    "    print(\"Loading labeled embeddings\")\n",
    "    labeled_embeddings, labeled_templates, _ = load_embeddings(labeled_embeddings_file)\n",
    "    print(\"Loading unlabeled embeddings\")\n",
    "    unlabeled_embeddings, _, image_paths = load_embeddings(unlabeled_embeddings_file, unlabeled=True)\n",
    "\n",
    "else:\n",
    "    # Load the phashes\n",
    "    labeled_embeddings, labeled_templates, _ = load_phases(labeled_phases_files)\n",
    "    labeled_embeddings = phash_to_bin(labeled_embeddings)\n",
    "    unlabeled_embeddings, _, image_paths = load_phases(unlabeled_phases_files)\n",
    "    unlabeled_embeddings = phash_to_bin(unlabeled_embeddings)\n",
    "\n",
    "print(\"finished loading\")\n",
    "print(type(unlabeled_embeddings))\n",
    "print(unlabeled_embeddings)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign labels to phrases\n",
    "------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For phashes we use BKTree to build the tree and Hamming-distance to find the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning labels: 100%|██████████| 1000/1000 [12:22<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "def hamming_distance_tuple(a, b):\n",
    "    \"\"\"Calculate the Hamming distance between two tuples, considering only the second element.\"\"\"\n",
    "    return hamming_distance(a[1], b[1])\n",
    "\n",
    "def hamming_distance(a, b):\n",
    "    # a and b are numpy arrays containing 0s and 1s\n",
    "    return np.count_nonzero(a != b)\n",
    "\n",
    "# Assuming that we have your data in numpy arrays\n",
    "labels = labeled_templates\n",
    "\n",
    "# Initialize BK-tree with the Hamming distance function\n",
    "bk_tree = BKTree(hamming_distance_tuple)\n",
    "\n",
    "# Populate BK-tree with labeled embeddings\n",
    "for i in range(labeled_embeddings.shape[0]):\n",
    "    bk_tree.add((labels[i], labeled_embeddings[i]))\n",
    "\n",
    "# Set a distance threshold for classifying a point as \"templateless\"\n",
    "distance_threshold = 28\n",
    "\n",
    "# Assign labels to the unlabeled points\n",
    "assigned_labels = []\n",
    "confidence_scores = []\n",
    "\n",
    "def assign_labels(unlabeled_embeddings):\n",
    "    for unlabeled_embedding in tqdm(unlabeled_embeddings, desc=\"Assigning labels\", total=unlabeled_embeddings.shape[0]):\n",
    "        try:\n",
    "            neighbors = bk_tree.find((None, unlabeled_embedding), distance_threshold)\n",
    "            if not neighbors:\n",
    "                # If no neighbors within distance threshold, classify as \"templateless\"\n",
    "                assigned_labels.append(\"templateless\")\n",
    "                confidence_scores.append(0.0)\n",
    "                continue\n",
    "\n",
    "            # Consider top 3 nearest neighbors\n",
    "            top_neighbors = sorted(neighbors, key=lambda x: x[0])[:3]\n",
    "\n",
    "            # Unpack labels and distances from the neighbors\n",
    "            nearest_labels = [label for distance, (label, _) in top_neighbors]\n",
    "            distances = np.array([distance for distance, _ in top_neighbors])\n",
    "            \n",
    "            # Apply weighted voting for label assignment\n",
    "            inverse_distances = 1 / (1 + distances)\n",
    "            weights = softmax(inverse_distances)\n",
    "            weighted_votes = Counter()\n",
    "            for label, weight in zip(nearest_labels, weights):\n",
    "                weighted_votes[label] += weight\n",
    "\n",
    "            # Assign the label with the highest weighted votes\n",
    "            most_common_label, highest_weighted_vote = weighted_votes.most_common(1)[0]\n",
    "            assigned_labels.append(most_common_label)\n",
    "\n",
    "            # Use the highest weighted vote as the confidence score\n",
    "            confidence_scores.append(highest_weighted_vote)\n",
    "        except KeyboardInterrupt:\n",
    "            return assigned_labels, confidence_scores\n",
    "    return assigned_labels, confidence_scores\n",
    "\n",
    "assigned_labels, confidence_scores = assign_labels(unlabeled_embeddings)    \n",
    "# Convert lists to numpy arrays\n",
    "assigned_labels = np.array(assigned_labels)\n",
    "confidence_scores = np.array(confidence_scores)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the phashes have been assigned a label, we can evaluate the quality of the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Silhouette Coefficient...\n",
      "Silhouette Coefficient: -0.02024418170567639\n",
      "Calculating Calinski-Harabasz Index...\n",
      "Calinski-Harabasz Index: 2.4129534652132167\n",
      "Calculating Davies-Bouldin Index...\n",
      "Davies-Bouldin Index: 0.8584082928801149\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Convert labels to integers for the metrics functions\n",
    "label_to_int = {label: i for i, label in enumerate(set(assigned_labels))}\n",
    "labels_int = np.array([label_to_int[label] for label in assigned_labels])\n",
    "\n",
    "print('Calculating Silhouette Coefficient...')\n",
    "silhouette = silhouette_score(unlabeled_embeddings, labels_int)\n",
    "print('Silhouette Coefficient:', silhouette)\n",
    "\n",
    "print('Calculating Calinski-Harabasz Index...')\n",
    "calinski_harabasz = calinski_harabasz_score(unlabeled_embeddings, labels_int)\n",
    "print('Calinski-Harabasz Index:', calinski_harabasz)\n",
    "\n",
    "print('Calculating Davies-Bouldin Index...')\n",
    "davies_bouldin = davies_bouldin_score(unlabeled_embeddings, labels_int)\n",
    "print('Davies-Bouldin Index:', davies_bouldin)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we can save the results to a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17436,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "np.save('assigned_labels.npy', assigned_labels)\n",
    "np.save('confidence_scores.npy', confidence_scores)\n",
    "\n",
    "\n",
    "# Step 5: Save results to JSON file\n",
    "print(image_paths.shape)\n",
    "\n",
    "# Collecting the results\n",
    "results = dict()\n",
    "\n",
    "for path, label, confidence in zip(image_paths, assigned_labels, confidence_scores):\n",
    "    if label not in results:\n",
    "        results[label] = {\"cluster_name\": label, \"images\": {}}\n",
    "    results[label][\"images\"][path] = float(confidence)  # convert numpy float to Python float\n",
    "\n",
    "results['metrics'] = {\n",
    "    'silhouette_score': silhouette,\n",
    "    'calinski_harabasz_score': calinski_harabasz,\n",
    "    'davies_bouldin_score': davies_bouldin,\n",
    "}\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('../outputs/clusters/jsons/phash_results.json', 'w') as f:\n",
    "    json.dump(list(results.values()), f, indent=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign labels to reddit embeddings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labeled embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Murgi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\function_base.py:5071: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = asarray(arr)\n"
     ]
    }
   ],
   "source": [
    "from vptree import VPTree\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.spatial import distance\n",
    "\n",
    "def cosine_similarity_tuple(a, b):\n",
    "    \"\"\"Calculate the cosine similarity between two tuples, considering only the second element.\"\"\"\n",
    "    return distance.cosine(a[1], b[1])\n",
    "\n",
    "# Load the embeddings\n",
    "print(\"Loading labeled embeddings\")\n",
    "labeled_embeddings, labeled_templates, _ = load_embeddings(labeled_embeddings_file)\n",
    "\n",
    "# Assuming that we have your data in numpy arrays\n",
    "labels = labeled_templates\n",
    "\n",
    "# Normalize embeddings to have unit norm, this makes cosine similarity work as expected\n",
    "labeled_embeddings = normalize(labeled_embeddings, norm='l2', axis=1)\n",
    "\n",
    "# Initialize VP-tree with the cosine similarity function\n",
    "vp_tree = VPTree(list(zip(labels, labeled_embeddings)), cosine_similarity_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 2048)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading unlabeled embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Assigning labels: 100%|██████████| 200/200 [00:14<00:00, 14.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading unlabeled embeddings\")\n",
    "unlabeled_embeddings, _, image_paths = load_embeddings(unlabeled_embeddings_file, unlabeled=True)\n",
    "\n",
    "# Set a distance threshold for classifying a point as \"templateless\"\n",
    "# The range of cosine similarity is [-1, 1], so the distance threshold should be in the range [0, 2]\n",
    "distance_threshold = 0.32\n",
    "\n",
    "# Assign labels to the unlabeled points\n",
    "assigned_labels = []\n",
    "confidence_scores = []\n",
    "\n",
    "for unlabeled_embedding in tqdm(unlabeled_embeddings, desc=\"Assigning labels\", total=unlabeled_embeddings.shape[0]):\n",
    "    neighbors = vp_tree.get_all_in_range((None, unlabeled_embedding), distance_threshold)\n",
    "    if not neighbors:\n",
    "        # If no neighbors within distance threshold, classify as \"templateless\"\n",
    "        assigned_labels.append(\"templateless\")\n",
    "        confidence_scores.append(0.0)\n",
    "        continue\n",
    "\n",
    "    # Consider top 3 nearest neighbors\n",
    "    top_neighbors = sorted(neighbors, key=lambda x: x[0])[:3]\n",
    "\n",
    "\n",
    "    # Unpack labels and distances from the neighbors\n",
    "    nearest_labels = [label for distance, (label, _) in top_neighbors]\n",
    "    distances = np.array([distance for distance, _ in top_neighbors])\n",
    "    \n",
    "    # Apply weighted voting for label assignment\n",
    "    inverse_distances = 1 / (1 + distances)\n",
    "    weights = softmax(inverse_distances)\n",
    "    weighted_votes = Counter()\n",
    "    for label, weight in zip(nearest_labels, weights):\n",
    "        weighted_votes[label] += weight\n",
    "\n",
    "    # Assign the label with the highest weighted votes\n",
    "    most_common_label, highest_weighted_vote = weighted_votes.most_common(1)[0]\n",
    "    assigned_labels.append(most_common_label)\n",
    "\n",
    "    # Use the highest weighted vote as the confidence score\n",
    "    confidence_scores.append(highest_weighted_vote)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "assigned_labels = np.array(assigned_labels)\n",
    "confidence_scores = np.array(confidence_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Convert labels to integers for the metrics functions\n",
    "label_to_int = {label: i for i, label in enumerate(set(assigned_labels))}\n",
    "labels_int = np.array([label_to_int[label] for label in assigned_labels])\n",
    "\n",
    "print('Calculating Silhouette Coefficient...')\n",
    "silhouette = silhouette_score(unlabeled_embeddings, labels_int)\n",
    "print('Silhouette Coefficient:', silhouette)\n",
    "\n",
    "print('Calculating Calinski-Harabasz Index...')\n",
    "calinski_harabasz = calinski_harabasz_score(unlabeled_embeddings, labels_int)\n",
    "print('Calinski-Harabasz Index:', calinski_harabasz)\n",
    "\n",
    "print('Calculating Davies-Bouldin Index...')\n",
    "davies_bouldin = davies_bouldin_score(unlabeled_embeddings, labels_int)\n",
    "print('Davies-Bouldin Index:', davies_bouldin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of templateless images: 2\n",
      "['/storage/kym-datasets/Memes2023_splitted_resized/finetuning/val/ytmnd/ytmnd_15.jpg'\n",
      " '/storage/kym-datasets/Memes2023_splitted_resized/finetuning/val/yume-nikki/yume-nikki_2.png']\n"
     ]
    }
   ],
   "source": [
    "# Count the number of templateless images\n",
    "num_templateless = np.sum(assigned_labels == \"templateless\")\n",
    "print(f\"Number of templateless images: {num_templateless}\")\n",
    "\n",
    "# Get the index of templateless images\n",
    "templateless_indices = np.where(assigned_labels == \"templateless\")[0]\n",
    "\n",
    "# Get the image paths of templateless images\n",
    "templateless_image_paths = image_paths[templateless_indices]\n",
    "\n",
    "print(templateless_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17436,)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "\n",
    "np.save('assigned_labels.npy', assigned_labels)\n",
    "np.save('confidence_scores.npy', confidence_scores)\n",
    "\n",
    "\n",
    "# Step 5: Save results to JSON file\n",
    "print(image_paths.shape)\n",
    "\n",
    "# Collecting the results\n",
    "results = dict()\n",
    "\n",
    "for path, label, confidence in zip(image_paths, assigned_labels, confidence_scores):\n",
    "    if label not in results:\n",
    "        results[label] = {\"cluster_name\": label, \"images\": {}}\n",
    "    results[label][\"images\"][path] = float(confidence)  # convert numpy float to Python float\n",
    "\n",
    "results['metrics'] = {\n",
    "    'silhouette_score': silhouette,\n",
    "    'calinski_harabasz_score': calinski_harabasz,\n",
    "    'davies_bouldin_score': davies_bouldin,\n",
    "}\n",
    "\n",
    "# Save the results to a JSON file\n",
    "with open('../outputs/clusters/jsons/embeddings_results.json', 'w') as f:\n",
    "    json.dump(list(results.values()), f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
